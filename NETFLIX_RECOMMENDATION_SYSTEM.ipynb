{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakshiharde/Netflix_Movie_And_Tvshows_RecommendationSystem/blob/main/NETFLIX_RECOMMENDATION_SYSTEM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Netflix Movies and TV shows Recommendation System\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n",
        "\n",
        "In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming serviceâ€™s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Initially i have start with understanding the dataset, then i clean the data to make analysis ready.\n",
        "\n",
        "Explore the data and understand the behaviour of the same.\n",
        "\n",
        "Then i have prepare the dataset for creating clusters by various parameters wherein i can remove stop words, white spaces numbers etc. so that i can get important words and based on that i shall form clusters.\n",
        "\n",
        "Later i have used the silhouette method and k-means elbow method to find optimal number of clusters and built recommender system by cosine similarity and recommended top ten movies."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix=pd.read_csv('/content/drive/MyDrive/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ],
      "metadata": {
        "id": "EnCHOuUQPXcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "netflix.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix.tail()"
      ],
      "metadata": {
        "id": "vrB67NSQP93Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "netflix.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "netflix.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix.describe()"
      ],
      "metadata": {
        "id": "ZJD1btfYQPo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "netflix.duplicated().sum()\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "netflix.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dataset , the director colummn has 2389 missing values , cast has 718 missing values and country has 507 missing values , date_added has 10 values and rating has 7 value missing"
      ],
      "metadata": {
        "id": "jTFFNvKAQoKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "netflix.isnull().sum().plot(kind='bar')\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SynaXoj_tLIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "netflix.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "netflix.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "netflix.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "netflix.isnull().sum()\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Handling Null Values\n",
        "netflix['cast'].fillna(value='No cast',inplace=True)\n",
        "netflix['country'].fillna(value=netflix['country'].mode()[0],inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "fYJ1mj75Rxpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix.dropna(subset=['date_added','rating'],inplace=True)\n"
      ],
      "metadata": {
        "id": "m6hEewivTz9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the null values in director.\n",
        "netflix['director']=netflix['director'].fillna('')\n"
      ],
      "metadata": {
        "id": "MLg6s29rT3pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix.isnull().sum()"
      ],
      "metadata": {
        "id": "yR_sb2VUR3zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. How many TV shows and movies are on Netflix?"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='type',data=netflix,hue='type')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix has 5372 movies and 2398 TV shows, there are more number movies on Netflix than TV shows."
      ],
      "metadata": {
        "id": "468CCCxDWTkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the most common rating for movies and TV shows on Netflix?"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "rating_counts = netflix['rating'].value_counts()\n",
        "plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%')\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='rating', data=netflix)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6nX6aqIYYN59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By analyzing these charts, you can gain insights into the content available on Netflix and the preferences of its audience. For example:\n",
        "\n",
        "\n",
        "*   Content Strategy: If you find that certain rating categories are underrepresented, Netflix could consider adding more content with those ratings to cater to a wider audience.\n",
        "*   Parental Controls: The distribution of ratings can inform the development of parental control features, ensuring that children are only exposed to age-appropriate content.\n",
        "\n",
        "*   Marketing and Recommendations: Understanding the popularity of different rating categories can help Netflix tailor its marketing campaigns and recommendation algorithms to better target specific audience segments.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How does the distribution of release years for movies compare to that of TV shows?"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# For movies\n",
        "movie_year_counts = netflix[netflix['type'] == 'Movie']['release_year'].value_counts().reset_index()\n",
        "movie_year_counts.columns = ['release_year', 'count']\n",
        "fig_movie = px.bar(movie_year_counts.head(10), x='release_year', y='count',\n",
        "                     title='Top 10 Most Common Release Years for Movies',\n",
        "                     labels={'release_year': 'Release Year', 'count': 'Count'})\n",
        "fig_movie.show()\n",
        "\n",
        "# For TV shows\n",
        "tvshow_year_counts = netflix[netflix['type'] == 'TV Show']['release_year'].value_counts().reset_index()\n",
        "tvshow_year_counts.columns = ['release_year', 'count']\n",
        "fig_tvshow = px.bar(tvshow_year_counts.head(10), x='release_year', y='count',\n",
        "                      title='Top 10 Most Common Release Years for TV Shows',\n",
        "                      labels={'release_year': 'Release Year', 'count': 'Count'})\n",
        "fig_tvshow.show()"
      ],
      "metadata": {
        "id": "O_Y03ONyyCVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Combine movie and TV show data\n",
        "movie_year_counts = netflix[netflix['type'] == 'Movie']['release_year'].value_counts().reset_index()\n",
        "movie_year_counts.columns = ['release_year', 'count']\n",
        "movie_year_counts['type'] = 'Movie'\n",
        "\n",
        "tvshow_year_counts = netflix[netflix['type'] == 'TV Show']['release_year'].value_counts().reset_index()\n",
        "tvshow_year_counts.columns = ['release_year', 'count']\n",
        "tvshow_year_counts['type'] = 'TV Show'\n",
        "\n",
        "combined_counts = pd.concat([movie_year_counts, tvshow_year_counts])\n",
        "\n",
        "# Create line plot with different colors\n",
        "fig = px.line(combined_counts, x='release_year', y='count', color='type',\n",
        "              title='Most Common Release Years for Movies and TV Shows',\n",
        "              labels={'release_year': 'Release Year', 'count': 'Count', 'type': 'Type'})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "OesIsP5I0zPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Which month sees the highest number of content additions? What factors might contribute to this peak?"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "netflix"
      ],
      "metadata": {
        "id": "eMlvL00q1Se8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "netflix['month'] = pd.DatetimeIndex(netflix['date_added']).month\n",
        "netflix.head()"
      ],
      "metadata": {
        "id": "XTaDVknY3lrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the Countplot\n",
        "plt.figure(figsize=(10,10))\n",
        "ax=sns.countplot(x='month',data=netflix,palette='viridis')\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Count of Movies and TV Shows Added to Netflix by Month\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5Mjut8O93w9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "\n",
        "sns.countplot(x='month', hue='type',lw=5, data=netflix, ax=ax)"
      ],
      "metadata": {
        "id": "cwByxD9BsTGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Peak Release Months: This could indicate strategic release periods targeted at specific viewer behaviors or industry trends. For example, if you see a spike in releases during the holiday season (November/December), it could suggest Netflix capitalizes on increased viewership during those times.\n",
        "*   Content Slumps:  any months with significantly lower content additions. These periods might be due to production cycles, industry events, or strategic decisions to focus releases elsewhere.\n",
        "\n"
      ],
      "metadata": {
        "id": "53XOJ_Ex2PtX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Who are the most frequently appearing actors on Netflix?"
      ],
      "metadata": {
        "id": "9e7jsNml5HqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the 'cast' column and flatten the list\n",
        "all_actors = netflix['cast'].str.split(', ').explode()\n",
        "\n",
        "all_actors_dropped=all_actors.dropna()\n",
        "\n",
        "# Count the occurrences of each actor\n",
        "actor_counts = all_actors.value_counts()\n",
        "\n",
        "# Display the top 10 actors\n",
        "print(actor_counts.head(10))"
      ],
      "metadata": {
        "id": "ayzW1S8w5G6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "actor_counts.head(10).plot(kind='bar')\n",
        "plt.title('Top 10 Actors on Netflix')\n",
        "plt.xlabel('Actor')\n",
        "plt.ylabel('Number of Appearances')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s3Po_cmtqEOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(actor_counts)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 8), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z24pZqjLtGEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Does the dominance of US-produced content reflect global viewer preferences, or is it a result of Netflix's origins and initial focus?"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split, explode, and count country occurrences\n",
        "country_counts = netflix['country'].str.split(', ').explode().value_counts()\n",
        "\n",
        "# Display the top 10 countries\n",
        "print(country_counts.head(10))\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "country_counts.head(10).sort_values(ascending=True).plot(kind='barh', color=plt.cm.Paired(np.arange(10)))\n",
        "plt.title('Top 10 Countries Producing Content on Netflix')\n",
        "plt.ylabel('Country')\n",
        "plt.xlabel('Number of Movies and TV Shows')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gZfnIAzkuUOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99', '#c2c2f0','#ffb3e6', '#c2d6d6', '#e6b3b3', '#b3e6cc', '#ffff99']\n",
        "# Create a pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "country_counts.head(10).plot(kind='pie', autopct='%1.1f%%', startangle=90, colors=colors)\n",
        "plt.title('Top 10 Countries Producing Content on Netflix')\n",
        "plt.ylabel('')  # Hide the default y-axis label\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9Ye3fBU1wQBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The United States dominates Netflix content production with a staggering 2555 titles, followed by India with 923 and the United Kingdom with 397. Other countries contribute significantly less, indicating a concentrated production landscape led by these major players."
      ],
      "metadata": {
        "id": "TJ22ThXtu9Gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the distribution of Movie Durations\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.distplot(netflix['duration'].str.extract('(\\d+)'),kde=False, color=['red'])\n",
        "plt.title('Distplot with Normal distribution for Movies',fontweight=\"bold\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tv_shows=netflix[netflix['type']=='TV Show']\n",
        "movies=netflix[netflix['type']=='Movie']"
      ],
      "metadata": {
        "id": "yJR4RJNMz16W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the distribution of TV SHOWS\n",
        "plt.figure(figsize=(30,6))\n",
        "plt.title(\"Distribution of TV Shows duration\",fontweight='bold')\n",
        "sns.countplot(x=tv_shows['duration'],data=tv_shows,order = tv_shows['duration'].value_counts().index,palette='viridis')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JWKlcHA2zmcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How can Netflix leverage this genre data to improve its recommendation algorithms and personalize user experiences?"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split, explode, and count genre occurrences\n",
        "genre_counts = netflix['listed_in'].str.split(', ').explode().value_counts()\n",
        "\n",
        "# Display the top 10 genres\n",
        "print(genre_counts.head(10))"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "genre_counts.head(10).plot(kind='bar', color='skyblue')\n",
        "plt.title('Top 10 Genres on Netflix')\n",
        "plt.xlabel('Genre')\n",
        "plt.ylabel('Number of Movies and TV Shows')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uT8ZZFRHxzjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "International Movies, Dramas, and Comedies are the most prevalent genres on Netflix, suggesting a strong viewer preference for these categories.\n",
        "The popularity of international movies and TV shows suggests a strong focus on acquiring and producing content from diverse regions, catering to a global audience."
      ],
      "metadata": {
        "id": "qFSONyAcy2JM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the most common words used in Netflix show titles?"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Combine all titles into a single string\n",
        "all_titles = ' '.join(netflix['title'].astype(str))\n",
        "\n",
        "# Split the string into individual words\n",
        "words = all_titles.lower().split()\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Display the most common words\n",
        "print(word_counts.most_common(10))"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "Uy9TSVn4fWQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "ps=PorterStemmer()\n",
        "import string"
      ],
      "metadata": {
        "id": "W__JuKzXfmuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment_words = ''\n",
        "\n",
        "# Remove The Stopwords\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# Iterate Through The Column\n",
        "for val in netflix.title:\n",
        "\n",
        "    # Typecaste Each Val to String\n",
        "    val = str(val)\n",
        "\n",
        "    # Split The Value\n",
        "    tokens = val.split()\n",
        "\n",
        "    # Converts Each Token into lowercase\n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = tokens[i].lower()\n",
        "\n",
        "    comment_words += \" \".join(tokens)+\" \"\n",
        "\n",
        "# Set Parameters\n",
        "wordcloud = WordCloud(width = 1000, height = 500,\n",
        "                background_color ='white',\n",
        "                stopwords = stopwords,\n",
        "                min_font_size = 10,\n",
        "                max_words = 1000,\n",
        "                colormap = 'gist_heat_r').generate(comment_words)\n",
        "\n",
        "plt.figure(figsize = (6,6), facecolor = None)\n",
        "plt.title('Most Used Words In Shows Title', fontsize = 15, pad=20)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JkCVRpwoixaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most common words in Netflix titles heavily emphasize personal relationships and everyday experiences, suggesting a focus on relatable and emotionally resonant stories.\n",
        "Action and thriller titles frequently use words like 'devill', 'dead', and 'war', indicating a focus on high-stakes plots and suspense."
      ],
      "metadata": {
        "id": "RIeLYIdsiuFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# who are the top 10 movies and tv shows actors on netflix?"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cast = netflix['cast'].str.split(', ', expand=True).stack()\n",
        "\n",
        "# top actors name who play highest role in movie/show.\n",
        "cast.value_counts()\n"
      ],
      "metadata": {
        "id": "DZ_RrzL-l7ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cast =cast[cast != 'No cast']\n",
        "cast.head()"
      ],
      "metadata": {
        "id": "s5huvs0klbnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualization of top 10 actors of movie and tv show on netflix\n",
        "fig,ax = plt.subplots(1,2, figsize=(14,5))\n",
        "\n",
        "# seperating TV shows actor from cast column\n",
        "top_TVshows_actor = netflix[netflix['type']=='TV Show']['cast'].str.split(', ', expand=True).stack()\n",
        "top_TVshows_actor =top_TVshows_actor[top_TVshows_actor != 'No cast']\n",
        "# plotting actor who appeared in highest number of TV Show\n",
        "a = top_TVshows_actor.value_counts().head(10).plot(kind='barh', ax=ax[0],color='purple')\n",
        "a.set_title('Top 10 TV shows actors', size=15)\n",
        "\n",
        "# seperating movie actor from cast column\n",
        "top_movie_actor = netflix[netflix['type']=='Movie']['cast'].str.split(', ', expand=True).stack()\n",
        "top_movie_actor =top_movie_actor[top_movie_actor != 'No cast']\n",
        "# plotting actor who appeared in highest number of Movie\n",
        "b = top_movie_actor.value_counts().head(10).plot(kind='barh', ax=ax[1],color='blue')\n",
        "b.set_title('Top 10 Movie actors', size=15)\n",
        "\n",
        "plt.tight_layout(pad=1.2, rect=[0, 0, 0.95, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bHVACs-TmOi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the TV shows category, the actor with the highest appearance is Takahiro Sakurai. In the movies category, the actor with the highest appearance is Anupam Kher."
      ],
      "metadata": {
        "id": "9mGMl-JMnIpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top 15 director who directed highest number of movies and TV show on Netflix"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directors_list = netflix.director.value_counts().reset_index().head(15)[1:]\n",
        "directors_list.rename(columns={'index':'count', 'director':'directors name'}, inplace=True)\n",
        "\n",
        "# Create a bar chart using Plotly\n",
        "fig = px.bar(directors_list, x='directors name', y='count', text_auto=True)\n",
        "\n",
        "# Generate a list of 25 unique color codes using seaborn\n",
        "color_palette = sns.color_palette('bright', n_colors=15).as_hex()\n",
        "fig.update_traces(marker_color=color_palette)\n",
        "\n",
        "# Add a title and adjust the layout\n",
        "fig.update_layout(\n",
        "    title={\n",
        "        'text': 'Top 25 directors with highest number of Movies and Tv Shows.',\n",
        "        'y': 0.95,\n",
        "        'x': 0.5,\n",
        "        'xanchor': 'center',\n",
        "        'yanchor': 'top'},\n",
        "    autosize=False,\n",
        "    width=1200,\n",
        "    height=500\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = {\n",
        "    'TV-PG': 'Older Kids',\n",
        "    'TV-MA': 'Adults',\n",
        "    'TV-Y7-FV': 'Older Kids',\n",
        "    'TV-Y7': 'Older Kids',\n",
        "    'TV-14': 'Teens',\n",
        "    'R': 'Adults',\n",
        "    'TV-Y': 'Kids',\n",
        "    'NR': 'Adults',\n",
        "    'PG-13': 'Teens',\n",
        "    'TV-G': 'Kids',\n",
        "    'PG': 'Older Kids',\n",
        "    'G': 'Kids',\n",
        "    'UR': 'Adults',\n",
        "    'NC-17': 'Adults'\n",
        "}\n",
        "netflix['target_ages'] = netflix['rating'].replace(ratings)"
      ],
      "metadata": {
        "id": "UrnK--UByu7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix['count'] = 1\n",
        "data = netflix.groupby('country')[['count']].sum().sort_values(by='count',ascending=False).reset_index()[:10]\n",
        "data = data['country']\n",
        "\n",
        "netflix_heatmap = netflix.loc[netflix['country'].isin(data)]\n",
        "netflix_heatmap = pd.crosstab(netflix_heatmap['country'],netflix_heatmap['target_ages'],normalize = \"index\").T\n",
        "netflix_heatmap"
      ],
      "metadata": {
        "id": "bZ7W2c1RxwIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the heatmap\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "\n",
        "country_order2 = ['United States', 'India', 'United Kingdom', 'Canada', 'Japan', 'France', 'South Korea', 'Spain']\n",
        "\n",
        "age_order = ['Adults', 'Teens', 'Older Kids', 'Kids']\n",
        "\n",
        "sns.heatmap(netflix_heatmap.loc[age_order,country_order2],cmap=\"PRGn\",square=True, linewidth=2.5,cbar=False,\n",
        "            annot=True,fmt='1.0%',vmax=.6,vmin=0.05,ax=ax,annot_kws={\"fontsize\":12})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "usyTjaoDxk1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix['count'] = 1\n",
        "data1 = netflix.groupby('listed_in')[[ 'count']].sum().sort_values(by='count', ascending=False).reset_index()[:10]\n",
        "data1 = data1['listed_in']\n"
      ],
      "metadata": {
        "id": "KIQshjKdkx79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_heatmap1 = netflix.loc[netflix['listed_in'].isin(data1)]\n",
        "df_heatmap1 = pd.crosstab(df_heatmap1['listed_in'],df_heatmap1['target_ages'],normalize = \"index\").T\n",
        "df_heatmap1"
      ],
      "metadata": {
        "id": "b-JMZMudlU_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "\n",
        "top=['Documentaries', 'Stand-Up Comedy', 'Dramas, International Movies',\n",
        "       'Comedies, Dramas, International Movies',\n",
        "       'Dramas, Independent Movies, International Movies', \"Kids' TV\",\n",
        "       'Children & Family Movies', 'Documentaries, International Movies',\n",
        "       'Children & Family Movies, Comedies',\n",
        "       'Comedies, International Movies']\n",
        "age_order = ['Adults', 'Teens', 'Older Kids', 'Kids']\n",
        "\n",
        "sns.heatmap(data=df_heatmap1.loc[age_order, top],\n",
        "            cmap='YlGnBu',\n",
        "            square=True,\n",
        "            linewidth=2.5,\n",
        "            cbar=False,\n",
        "            annot=True,\n",
        "            fmt='1.0%',\n",
        "            vmax=.6,\n",
        "            vmin=0.05,\n",
        "            ax=ax,\n",
        "            annot_kws={\"fontsize\": 12})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ygyU3rBKlmWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These visualisations show the content's country of origin, which include both Movies and TVs shows. Top of the list of nations were the US and India. A few countries, including Australia, Taiwan, and Brazil, produce little Netflix content.\n",
        "\n",
        "From the heatmap,the US and UK are very similar to the Netflix target age group, although they differ greatly from such as India or Japan."
      ],
      "metadata": {
        "id": "s65UbEX0zJky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Select relevant numeric columns for the pair plot\n",
        "columns_for_pairplot = ['release_year', 'duration']  # Add or remove columns as needed\n",
        "\n",
        "# Create the pair plot\n",
        "sns.pairplot(netflix[columns_for_pairplot], diag_kind='kde')\n",
        "plt.suptitle(\"Pair Plot of Selected Netflix Features\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#making copy of df_clean_frame\n",
        "netflix_hypothesis=netflix.copy()\n",
        "#head of df_hypothesis\n",
        "netflix_hypothesis.head()"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filtering movie from Type_of_show column\n",
        "netflix_hypothesis = netflix_hypothesis[netflix_hypothesis[\"type\"] == \"Movie\"]"
      ],
      "metadata": {
        "id": "TUEMjFDPl-_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with respect to each ratings assigning it into group of categories\n",
        "ratings_ages = {\n",
        "    'TV-PG': 'Older Kids',\n",
        "    'TV-MA': 'Adults',\n",
        "    'TV-Y7-FV': 'Older Kids',\n",
        "    'TV-Y7': 'Older Kids',\n",
        "    'TV-14': 'Teens',\n",
        "    'R': 'Adults',\n",
        "    'TV-Y': 'Kids',\n",
        "    'NR': 'Adults',\n",
        "    'PG-13': 'Teens',\n",
        "    'TV-G': 'Kids',\n",
        "    'PG': 'Older Kids',\n",
        "    'G': 'Kids',\n",
        "    'UR': 'Adults',\n",
        "    'NC-17': 'Adults'\n",
        "}\n",
        "\n",
        "netflix_hypothesis['target_ages'] = netflix_hypothesis['rating'].replace(ratings_ages)\n",
        "#let's see unique target ages\n",
        "netflix_hypothesis['target_ages'].unique()\n"
      ],
      "metadata": {
        "id": "9YoItTLFmFHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix_hypothesis['target_ages'] = pd.Categorical(netflix_hypothesis['target_ages'], categories=['Kids', 'Older Kids', 'Teens', 'Adults'])\n",
        "\n",
        "netflix_hypothesis['duration'] = netflix_hypothesis['duration'].astype(str)  # Convert to string type\n",
        "netflix_hypothesis['duration'] = netflix_hypothesis['duration'].str.extract('(\\d+)')\n",
        "netflix_hypothesis['duration'] = pd.to_numeric(netflix_hypothesis['duration'])\n",
        "\n",
        "netflix_hypothesis.head(3)\n",
        "\n"
      ],
      "metadata": {
        "id": "fntt5zPumOJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#group_by duration and target_ages\n",
        "group_by_= netflix_hypothesis[['duration','target_ages']].groupby(by='target_ages')\n",
        "#mean of group_by variable\n",
        "group=group_by_.mean().reset_index()\n",
        "group\n"
      ],
      "metadata": {
        "id": "CiRQYb3FmWpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#group_by duration and target_ages\n",
        "group_by_= netflix_hypothesis[['duration','target_ages']].groupby(by='target_ages')\n",
        "#mean of group_by variable\n",
        "group=group_by_.mean().reset_index()\n",
        "group\n",
        "\n",
        "#In A and B variable grouping values\n",
        "A= group_by_.get_group('Kids')\n",
        "B= group_by_.get_group('Older Kids')\n",
        "\n",
        "# Convert 'duration' to numeric before calculating mean and std\n",
        "A['duration'] = pd.to_numeric(A['duration'])\n",
        "B['duration'] = pd.to_numeric(B['duration'])\n",
        "\n",
        "#mean and std. calutation for kids and older kids variables\n",
        "M1 = A['duration'].mean() # Calculate mean of the 'duration' column\n",
        "S1 = A['duration'].std()\n",
        "\n",
        "M2= B['duration'].mean()\n",
        "S2 = B['duration'].std()\n",
        "\n",
        "print('Mean for movies rated for Kids {} \\n Mean for  movies rated for older kids {}'.format(M1,M2))\n",
        "print('Std for  movies rated for Older Kids {} \\n Std for  movies rated for kids {}'.format(S2,S1))"
      ],
      "metadata": {
        "id": "IlJ40ESom5PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import stats\n",
        "from scipy import stats\n",
        "#length of groups and DOF\n",
        "n1 = len(A)\n",
        "n2= len(B)\n",
        "print(n1,n2)\n",
        "\n",
        "dof = n1+n2-2\n",
        "print('dof',dof)\n",
        "\n",
        "sp_2 = ((n2-1)*S1**2  + (n1-1)*S2**2) / dof\n",
        "print('SP_2 =',sp_2)\n",
        "\n",
        "sp = np.sqrt(sp_2)\n",
        "print('SP',sp)\n",
        "\n",
        "#tvalue\n",
        "t_val = (M1-M2)/(sp * np.sqrt(1/n1 + 1/n2))\n",
        "print('tvalue',t_val) # Remove [0] to print the scalar value directly"
      ],
      "metadata": {
        "id": "JvBQDG-znp1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t-distribution\n",
        "stats.t.ppf(0.025,dof)"
      ],
      "metadata": {
        "id": "wTitglGVnpxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t-distribution\n",
        "stats.t.ppf(0.975,dof)"
      ],
      "metadata": {
        "id": "W5usaw5zn6T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-value is not in the range, the null hypothesis is rejected."
      ],
      "metadata": {
        "id": "XWd4slxIoa7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a result, movies rated for kids and older kids are not at least two hours long."
      ],
      "metadata": {
        "id": "IZbkuvq7odB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H1:The duration which is more than 90 mins are movies"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HO:The duration which is more than 90 mins are NOT movies"
      ],
      "metadata": {
        "id": "LtA20kfjotJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#making copy of df_clean_frame\n",
        "netflix_hypothesis=netflix.copy()\n",
        "#head of df_hypothesis\n",
        "netflix_hypothesis.head()"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix_hypothesis['duration']= netflix_hypothesis['duration'].str.extract('(\\d+)')\n",
        "netflix_hypothesis['duration'] = pd.to_numeric(netflix_hypothesis['duration'])"
      ],
      "metadata": {
        "id": "hHttf3Rpo-DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix_hypothesis['type'] = pd.Categorical(netflix_hypothesis['type'], categories=['Movie','TV Show'])\n",
        "#from duration feature extractin string part and after extracting Changing the object type to numeric\n",
        "#df_hypothesis['duration']= df_hypothesis['duration'].str.extract('(\\d+)')\n",
        "#df_hypothesis['duration'] = pd.to_numeric(df_hypothesis['duration'])\n",
        "#head of df_\n",
        "netflix_hypothesis.head(3)\n"
      ],
      "metadata": {
        "id": "xs-k1cHvpEIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix_hypothesis['type'] = pd.Categorical(netflix_hypothesis['type'], categories=['Movie','TV Show'])"
      ],
      "metadata": {
        "id": "XPlF3NS8pKFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "#group_by duration and TYPE\n",
        "group_by_= netflix_hypothesis[['duration','type']].groupby(by='type')\n",
        "#mean of group_by variable\n",
        "group1=group_by_.mean().reset_index()\n",
        "group1"
      ],
      "metadata": {
        "id": "UlwxAuWMpPG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In A and B variable grouping values\n",
        "A= group_by_.get_group('Movie')['duration'] # Select only the 'duration' column\n",
        "B= group_by_.get_group('TV Show')['duration'] # Select only the 'duration' column\n",
        "\n",
        "#mean and std\n",
        "M1 = A.mean()\n",
        "S1 = A.std()\n",
        "\n",
        "M2= B.mean()\n",
        "S2 = B.std()\n",
        "\n",
        "print('Mean of Movie durations: {}'.format(M1)) # Format the output for clarity\n",
        "print('Mean of TV Show durations: {}'.format(M2))\n",
        "print('Std of Movie durations: {}'.format(S1))\n",
        "print('Std of TV Show durations: {}'.format(S2))"
      ],
      "metadata": {
        "id": "-8-mpXblpvla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import stats\n",
        "from scipy import stats\n",
        "#length of groups and DOF\n",
        "n1 = len(A)\n",
        "n2= len(B)\n",
        "print(n1,n2)\n",
        "\n",
        "dof = n1+n2-2\n",
        "print('dof',dof)\n",
        "\n",
        "sp_2 = ((n2-1)*S1**2  + (n1-1)*S2**2) / dof\n",
        "print('SP_2 =',sp_2)\n",
        "\n",
        "sp = np.sqrt(sp_2)\n",
        "print('SP',sp)\n",
        "\n",
        "#tvalue\n",
        "t_val = (M1-M2)/(sp * np.sqrt(1/n1 + 1/n2))\n",
        "print('tvalue',t_val) # Remove [0] to print the scalar value directly"
      ],
      "metadata": {
        "id": "bfmGRi-JqCYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-distribution\n",
        "     "
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#t-distribution\n",
        "stats.t.ppf(0.025,dof)"
      ],
      "metadata": {
        "id": "_BmcMMIdqdpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t-distribution\n",
        "stats.t.ppf(0.975,dof)"
      ],
      "metadata": {
        "id": "YKZOT8isqmdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining all the clustering attributes into a single column\n",
        "netflix['clustering'] = (netflix['director'] + ' ' + netflix['cast'] +' ' +\n",
        "                                 netflix['country'] +' ' + netflix['listed_in'] +\n",
        "                                 ' ' + netflix['description'])"
      ],
      "metadata": {
        "id": "cSE5IG0iq6HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix['clustering'][25]"
      ],
      "metadata": {
        "id": "sUeDb9Ofq-CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "# Lower Casing\n",
        "# Remove Punctuations\n",
        "# Remove URLs & Remove words and digits contain digits\n",
        "# Remove Stopwords\n",
        "# Remove White spaces\n",
        "# Rephrase Text\n",
        "# Tokenization\n",
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('all',quiet=True)\n",
        "from PIL import Image\n",
        "\n",
        "def transform_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Tokenize text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove non-alphanumeric characters\n",
        "    words = [word for word in words if word.isalnum()]\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    punctuation_set = set(string.punctuation)\n",
        "    words = [word for word in words if word not in stopwords_set and word not in punctuation_set]\n",
        "\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Join words into a string and return\n",
        "    return ' '.join(lemmatized_words)"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix['Clean_Text'] = netflix['clustering'].apply(transform_text)\n"
      ],
      "metadata": {
        "id": "AtdXZfMNvdNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix[\"Clean_Text\"][50]"
      ],
      "metadata": {
        "id": "wIr1jP3Bv4KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF combines two metrics: Term frequency (TF) and inverse document frequency (IDF).\n",
        "\n",
        "Term Frequency (TF): This metric measures the frequency of a term in a document. It assumes that the more often a term appears in a document, the more relevant it is to that document. It is calculated using the formula:\n",
        "\n",
        "TF(t, d) = (Number of times term t appears in document d) / (Total number of terms in document d)\n",
        "\n",
        "Inverse Document Frequency (IDF): This metric measures the importance of a term across a collection of documents. It gives higher weight to terms that appear less frequently in the entire collection. It is calculated using the formula:\n",
        "\n",
        "IDF(t) = log_e(Total number of documents / Number of documents containing term t)"
      ],
      "metadata": {
        "id": "ppJMFLLowmsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words = netflix.Clean_Text\n",
        "\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_vectorizer = TfidfVectorizer(max_features=20000)\n",
        "X= t_vectorizer.fit_transform(bag_of_words)\n"
      ],
      "metadata": {
        "id": "AFPugGVUZ3K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "t_vectorizer = TfidfVectorizer(max_features=20000)\n",
        "X= t_vectorizer.fit_transform(bag_of_words)"
      ],
      "metadata": {
        "id": "sf_DsujcwL9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)"
      ],
      "metadata": {
        "id": "2pR0WaSXwdID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "rIeAO5HIwg_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA to reduce the dimensionality of the dataset. PCA identifies the directions (principal components) along which the data varies the most. These components are ordered by the amount of variance they explain in the data."
      ],
      "metadata": {
        "id": "glLUP4FAwsTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "transformer = PCA()\n",
        "transformer.fit(X.toarray())"
      ],
      "metadata": {
        "id": "crqbQtdCv9kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "n_batches = 10  # Adjust based on your dataset size\n",
        "inc_pca = IncrementalPCA()\n",
        "for X_batch in np.array_split(X.toarray(), n_batches):\n",
        "    inc_pca.partial_fit(X_batch)\n",
        "X_transformed = inc_pca.transform(X.toarray())"
      ],
      "metadata": {
        "id": "y1XqkYwe8OIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Lets plot explained var v/s comp to check how many components to be considered.\n",
        " #explained var v/s comp\n",
        "# Add a grid to the plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(15,5), dpi=120)\n",
        "plt.plot(np.cumsum(inc_pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance')\n",
        "plt.axhline(y=0.95, color='r', linestyle='--',linewidth=2, label='95% Explained Variance')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kUC5_UBmbh9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot helps in determining the number of components to consider for dimensionality reduction. You can select the number of components where the cumulative explained variance reaches a satisfactory threshold, such as 95%. The point where the curve intersects or is closest to the threshold line can guide you in choosing the appropriate number of components for your analysis."
      ],
      "metadata": {
        "id": "1CxWVHebbqyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "from sklearn.decomposition import PCA\n",
        "# Create an instance of PCA with the desired explained variance ratio\n",
        "pca_tuned = PCA(n_components=0.95)\n",
        "# Fit the PCA model on the input data, X, which is converted to a dense array\n",
        "pca_tuned.fit(X.toarray())\n",
        "# Transform the input data, X, to its reduced dimensional representation\n",
        "X_transformed = pca_tuned.transform(X.toarray())\n",
        "# Print the shape of the transformed data to see the number of samples and transformed features\n",
        "print(X_transformed.shape)\n"
      ],
      "metadata": {
        "id": "zQFmvY-sbuOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_transformed"
      ],
      "metadata": {
        "id": "eYUKYpNRbz8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Sample the data (adjust n_samples as needed)\n",
        "X_sample = resample(X_transformed, n_samples=1000, random_state=5)\n",
        "\n",
        "# Initialize KMeans with parallelization\n",
        "model = KMeans(random_state=5)\n",
        "\n",
        "# Narrow down the k range\n",
        "visualizer = KElbowVisualizer(model, k=(8, 12), metric='silhouette', timings=False, locate_elbow=True)\n",
        "\n",
        "# Fit on the sampled data\n",
        "visualizer.fit(X_sample)\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "Sf379kJqJS8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "\n",
        "def silhouette_score_analysis(n):\n",
        "\n",
        "  for n_clusters in range(2,n):\n",
        "      km = KMeans (n_clusters=n_clusters, random_state=5)\n",
        "      preds = km.fit_predict(X_transformed)\n",
        "      centers = km.cluster_centers_\n",
        "\n",
        "      score = silhouette_score(X_transformed, preds, metric='euclidean')\n",
        "      print (\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))\n",
        "\n",
        "      visualizer = SilhouetteVisualizer(km)\n",
        "\n",
        "      visualizer.fit(X_transformed) # Fit the training data to the visualizer\n",
        "      visualizer.poof() # Draw/show/poof the data\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_score_analysis(15)"
      ],
      "metadata": {
        "id": "rNq3-cTYd69R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create a figure with a specific size and resolution\n",
        "plt.figure(figsize=(10, 6), dpi=120)\n",
        "\n",
        "# Initialize an empty list to store the within-cluster sum of squares (WCSS)\n",
        "wcss = []\n",
        "\n",
        "# Iterate over different numbers of clusters\n",
        "for i in range(1, 22):\n",
        "    # Create a KMeans model with default parameters\n",
        "    model = KMeans(random_state=0)\n",
        "\n",
        "    # Initialize the KMeans algorithm with specific parameters\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "\n",
        "    # Fit the KMeans algorithm to the transformed data\n",
        "    kmeans.fit(X_transformed)\n",
        "\n",
        "    # Append the WCSS to the list\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the number of clusters against the WCSS\n",
        "plt.plot(range(1, 22), wcss)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Create a figure with a larger size and resolution\n",
        "plt.figure(figsize=(20, 8), dpi=120)\n",
        "\n",
        "# Initialize a KMeans model with 15 clusters\n",
        "kmeans = KMeans(n_clusters=15, init='k-means++', random_state=9)\n",
        "\n",
        "# Fit the KMeans algorithm to the transformed data\n",
        "kmeans.fit(X_transformed)\n",
        "\n",
        "# Predict the labels of the clusters\n",
        "label = kmeans.fit_predict(X_transformed)\n",
        "\n",
        "# Get unique labels from the predictions\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "# Plot the results\n",
        "for i in unique_labels:\n",
        "    # Scatter plot the points belonging to each cluster\n",
        "    plt.scatter(X_transformed[label == i, 0], X_transformed[label == i, 1], label=i)\n",
        "\n",
        "# Display a legend to identify the clusters\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SXgtWQK9eIof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix['cluster_number'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "dkZliF87eTH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix.head(1)"
      ],
      "metadata": {
        "id": "zDXrEE7jeexL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of movies or TV shows in each cluster\n",
        "cluster_content_count = netflix['cluster_number'].value_counts().reset_index().rename(columns={'index': 'clusters', 'clusters': 'Movies/TV_Shows'})\n",
        "\n",
        "# Print the cluster content count\n",
        "print(cluster_content_count)\n"
      ],
      "metadata": {
        "id": "aX6RrXe0ehyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word cloud\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "def word_count(category):\n",
        "  print(\"Exploring Cluster\", category)\n",
        "  col_names = ['type','title','country','rating','listed_in','description']\n",
        "  for i in col_names:\n",
        "    df_word_cloud = netflix[['cluster_number',i]].dropna()\n",
        "    df_word_cloud = df_word_cloud[df_word_cloud['cluster_number']==category]\n",
        "    text = \" \".join(word for word in df_word_cloud[i])\n",
        "    # Create stopword list:\n",
        "    stopwords = set(STOPWORDS)\n",
        "  # Generate a word cloud image\n",
        "    wordcloud = WordCloud(stopwords=stopwords, background_color=\"#FFC0CB\",width=500,height=500).generate(text)\n",
        "  # Display the generated image:\n",
        "  # the matplotlib way:\n",
        "    plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    print(\"Looking for insights from\", i ,\"Movies/TV Shows\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "AA_aa9JNem6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_count(9)"
      ],
      "metadata": {
        "id": "KssRvawEetwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "ElCyKsGje55F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing stopwords\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "#Replace NaN with an empty string\n",
        "netflix['description'] = netflix['description'].fillna('')\n",
        "\n",
        "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
        "tfidf_matrix = tfidf.fit_transform(netflix['description'])\n",
        "\n",
        "#Output the shape of tfidf_matrix\n",
        "tfidf_matrix.shape\n"
      ],
      "metadata": {
        "id": "TE9eBZpqe9AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import linear_kernel\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "# Compute the cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
      ],
      "metadata": {
        "id": "jKUf6uAsfG23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim"
      ],
      "metadata": {
        "id": "tVwh762RfLFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = pd.Series(netflix.index, index=netflix['title']).drop_duplicates()\n"
      ],
      "metadata": {
        "id": "NykiMyyZfOXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recommendations(title, cosine_sim=cosine_sim):\n",
        "    idx = indices[title]\n",
        "\n",
        "    # Get the pairwsie similarity scores of all movies with that movie\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "\n",
        "    # Sort the movies based on the similarity scores\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the scores of the 10 most similar movies\n",
        "    sim_scores = sim_scores[1:11]\n",
        "    # Get the movie indices\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    # Return the top 10 most similar movies\n",
        "    return netflix['title'].iloc[movie_indices]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zgIEvzbifT2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix['title'][1:70]\n"
      ],
      "metadata": {
        "id": "934kXlWhfZRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_recommendations( '14 Cameras',cosine_sim)"
      ],
      "metadata": {
        "id": "I6hSVX4gfeHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   It is interesting to note that the majority of the content available on Netflix consists of movies. However, in recent years, the platform has been focusing more on TV shows.\n",
        "2.   Most of these shows are released either at the end or the beginning of the year.\n",
        "\n",
        "1.   The United States and India are among the top five countries that produce all of the available content on the platform. Additionally, out of the top ten actors with the maximum content, six of them are from India.\n",
        "2.    When it comes to content ratings, TV-MA tops the charts, indicating that mature content is more popular on Netflix.\n",
        "\n",
        "1.   The value of k=15 was found to be optimal for clustering the data, and it was used to group the content into ten distinct clusters.\n",
        "2.   Using this data, a Content based recommender system was created using cosine similarity, which provided recommendations for Movies and TV shows.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}